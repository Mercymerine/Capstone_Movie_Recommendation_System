{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLEYnMusPA/ngG/RPJr8aD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mercymerine/Capstone_Movie_Recommendation_System/blob/main/main_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install feedparser"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyE77B87QqzK",
        "outputId": "776fc10d-2d76-49ec-c05a-b32379c393b9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting sgmllib3k (from feedparser)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=a04ed19df1e76ecbc1e6d737116eec31c8bea2cce6c2625bb00ddd3a400cfa91\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser\n",
            "Successfully installed feedparser-6.0.11 sgmllib3k-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install schedule"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94RhFPYwQ6ZZ",
        "outputId": "f1cae8f6-0285-4f6d-afe8-1a48f1bf2e00"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting schedule\n",
            "  Downloading schedule-1.2.2-py3-none-any.whl.metadata (3.8 kB)\n",
            "Downloading schedule-1.2.2-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: schedule\n",
            "Successfully installed schedule-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ql0jahCVAUm",
        "outputId": "917da468-8701-4681-805b-d6cd16447661"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Downloading faiss_cpu-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJXwogPjVFFS",
        "outputId": "21c8acc0-005e-463e-cdd7-9e51fa4b5c0b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDTtwgcGOWHx",
        "outputId": "9a3799cf-9ed2-400d-e66c-ff102a575c7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RSS feed fetched and written to CSV at 2024-10-11 16:22:39\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-310db016f98e>:54: FutureWarning: Parsed string \"Fri, 11 Oct 2024 15:03:08 GM\" included an un-recognized timezone \"GM\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
            "  rss['Date'] = pd.to_datetime(rss['Date'], errors='coerce')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Link: https://www.usatoday.com/story/news/politics/elections/2024/10/11/obama-calls-out-black-men-kamala-harris-support/75625647007/, Title: obama call black man hesitance harris think sit president barack obama call oublack man reluctant support kamala harris suggest woman, Author: Joey Garrison\n",
            "Link: https://www.theledger.com/story/weather/hurricane/2024/10/11/lakeland-electric-says-it-could-be-7-days-duke-outages-grow-in-polk/75625587007/, Title: power restore polk lakeland electric make steady progress restore power say restoration seven day duke power outage grow polk county, Author: The Ledger\n",
            "Link: https://www.theguardian.com/us-news/live/2024/oct/11/trump-harris-us-elections-obama-latest-updates, Title: obama take trump lie fake strength urge man vote harris president scoff idea trump bullying show strength speech condemn hurricane lie ask ok, Author: the Guardian\n",
            "Link: https://www.aljazeera.com/news/2024/10/11/israeli-forces-again-target-un-peacekeepers-in-southern-lebanon, Title: israeli force target un peacekeeper southern lebanon attack sri lankan battalion naqoura come day indonesian peacekeeper injure, Author: Al Jazeera\n",
            "Link: https://www.bbc.com/news/articles/cwyl2x3ye8no, Title: beirut strike rescue worker search sign miss rubble attack thursday evening kill 22 wound 117 lebanese health ministry say, Author: bbc.com\n"
          ]
        }
      ],
      "source": [
        "import feedparser\n",
        "import pandas as pd\n",
        "import csv\n",
        "import schedule\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "import spacy\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import faiss\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# RSS Feed URL\n",
        "RSS_URL = 'https://rss.app/feeds/atZtRJTsJwJI7KSQ.xml'\n",
        "\n",
        "# Function to fetch RSS data\n",
        "def fetch_rss():\n",
        "    try:\n",
        "        feed = feedparser.parse(RSS_URL)\n",
        "        if not feed.entries:\n",
        "            print(\"No entries found in the RSS feed.\")\n",
        "            return\n",
        "\n",
        "        with open('rss_feed.csv', 'a', newline='', encoding='utf-8') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "\n",
        "            # Write the header if the file is empty\n",
        "            if csvfile.tell() == 0:\n",
        "                writer.writerow(['Date', 'Title', 'Author', 'Summary', 'Category', 'Link'])\n",
        "\n",
        "            for entry in feed.entries:\n",
        "                date = entry.published.split('T')[0] if 'published' in entry else 'N/A'\n",
        "                title = entry.title if 'title' in entry else 'N/A'\n",
        "                author = entry.author if 'author' in entry else 'N/A'\n",
        "                summary = entry.summary if 'summary' in entry else 'N/A'\n",
        "                soup = BeautifulSoup(summary, 'html.parser')\n",
        "                summary_text = soup.get_text()\n",
        "\n",
        "                categories = ', '.join([cat.term for cat in entry.tags]) if 'tags' in entry else 'N/A'\n",
        "                link = entry.link if 'link' in entry else 'N/A'\n",
        "\n",
        "                writer.writerow([date, title, author, summary_text, categories, link])\n",
        "\n",
        "        print(f\"RSS feed fetched and written to CSV at {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while fetching the RSS feed: {e}\")\n",
        "\n",
        "# Function to process the RSS data\n",
        "def process_rss_data():\n",
        "    rss = pd.read_csv('rss_feed.csv')  # Load the most recent data\n",
        "    rss['Date'] = pd.to_datetime(rss['Date'], errors='coerce')\n",
        "\n",
        "    # Clean and preprocess the DataFrame\n",
        "    rss.dropna(subset=['Title'], inplace=True)\n",
        "    rss['Summary'] = rss['Summary'].astype(str)\n",
        "    rss['Text'] = rss['Summary'].apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n",
        "    rss.drop(['Summary'], axis=1, inplace=True)\n",
        "    rss.drop_duplicates(inplace=True)\n",
        "\n",
        "    # Replace missing values\n",
        "    rss.fillna({\n",
        "        'Author': 'Unknown',\n",
        "        'Link': 'Not Found',\n",
        "        'Text': 'Text Not Found',\n",
        "        'Title': 'Title Not Found',\n",
        "        'Date': 'Date Not Found'\n",
        "    }, inplace=True)\n",
        "\n",
        "    # Create a combined text column\n",
        "    rss['combined_text'] = rss['Title'] + ' ' + rss['Text']\n",
        "\n",
        "    # Process text for NLP\n",
        "    rss['combined_text'] = rss['combined_text'].apply(lambda x: preprocess_text(x))\n",
        "\n",
        "    return rss\n",
        "\n",
        "# Text preprocessing function\n",
        "def preprocess_text(text):\n",
        "    doc = nlp(text.lower())\n",
        "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "\n",
        "# Function to convert text to vectors using TF-IDF\n",
        "def convert_to_vectors(df):\n",
        "    \"\"\"Converts the 'combined_text' column to TF-IDF vectors.\"\"\"\n",
        "    tfidf = TfidfVectorizer()\n",
        "    vectors = tfidf.fit_transform(df['combined_text'])\n",
        "    return vectors, tfidf\n",
        "\n",
        "# Function to perform Faiss search\n",
        "def faiss_search(vectors, query_vector, tfidf, top_n=10):\n",
        "    \"\"\"Performs Faiss search and returns the top_n results.\"\"\"\n",
        "    index = faiss.IndexFlatL2(vectors.shape[1])\n",
        "    index.add(vectors.toarray().astype('float32'))\n",
        "\n",
        "   # Transform the query using the same TF-IDF vectorizer\n",
        "    query_vector = tfidf.transform([query_vector]).toarray().astype('float32') # Transform the query\n",
        "    D, I = index.search(query_vector, top_n)\n",
        "\n",
        "    return I, D\n",
        "\n",
        "# Function to recommend articles based on processed data\n",
        "def recommend_articles(df, query, top_n=10):\n",
        "    query_processed = preprocess_text(query) # Preprocess the query\n",
        "    vectors, tfidf = convert_to_vectors(df)\n",
        "\n",
        "    I, D = faiss_search(vectors, query_processed, tfidf, top_n)\n",
        "\n",
        "    results = []\n",
        "    seen_titles = set()\n",
        "\n",
        "    for idx in I[0]:\n",
        "        row = df.iloc[idx]\n",
        "        final_score = 1 / (1 + D[0][I[0].tolist().index(idx)])  # Example scoring\n",
        "        title = row['combined_text']\n",
        "        if title not in seen_titles:\n",
        "            results.append((row['Link'], title, row['Author'], final_score))\n",
        "            seen_titles.add(title)\n",
        "\n",
        "    return sorted(results, key=lambda x: x[3], reverse=True)[:top_n]\n",
        "\n",
        "# Main function to automate the process\n",
        "def main():\n",
        "    fetch_rss()  # Fetch new data\n",
        "    processed_data = process_rss_data()  # Process the new data\n",
        "    results = recommend_articles(processed_data, query='Kenya', top_n=5)  # Example query\n",
        "\n",
        "    # Print the results\n",
        "    for link, text, author, score in results:\n",
        "        print(f\"Link: {link}, Title: {text}, Author: {author}\")\n",
        "\n",
        "# Schedule the pipeline to run every 5 minutes\n",
        "schedule.every(60).minutes.do(main)\n",
        "\n",
        "# Run the scheduled task\n",
        "while True:\n",
        "    schedule.run_pending()\n",
        "    time.sleep(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-h0ts3IkSjL0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}